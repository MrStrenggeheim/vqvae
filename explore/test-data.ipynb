{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from datasets.amos import AmosDataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_amos(path):\n",
    "    train = AmosDataset(path, split='train',\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(256),\n",
    "                            transforms.Normalize(0.5, 0.5)\n",
    "                        ]),\n",
    "                        index_range = range(0, 500)\n",
    "                        )\n",
    "\n",
    "    val = AmosDataset(path, split='val',\n",
    "                      transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(256),\n",
    "                            # transforms.Normalize(0.5, 0.5)\n",
    "                      ]),\n",
    "                      index_range=range(0, 500)\n",
    "                      )\n",
    "    return train, val\n",
    "    \n",
    "\n",
    "\n",
    "def data_loaders(train_data, val_data, batch_size):\n",
    "\n",
    "    train_loader = DataLoader(train_data,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    val_loader = DataLoader(val_data,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def calc_x_train_var(train_data):\n",
    "    print('Calculating x_train_var')\n",
    "    all_images = []\n",
    "    for img, _ in train_data:\n",
    "        img = img / 255.0\n",
    "        all_images.append(img.flatten())\n",
    "    all_images = np.concatenate(all_images)\n",
    "    return np.var(all_images)\n",
    "        \n",
    "\n",
    "def load_data_and_data_loaders(path, batch_size):\n",
    "    training_data, validation_data = load_amos(path)\n",
    "    training_loader, validation_loader = data_loaders(\n",
    "        training_data, validation_data, batch_size)\n",
    "    # x_train_var = calc_x_train_var(training_data)\n",
    "\n",
    "    return training_data, validation_data, training_loader, validation_loader#, x_train_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amos train data\n",
      "Loaded 26069 train images\n",
      "Loading Amos val data\n",
      "Loaded 15361 val images\n"
     ]
    }
   ],
   "source": [
    "tr, va, trl, val = load_data_and_data_loaders('/vol/aimspace/users/hunecke/diffusion/data/amos_slices/', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trl = iter(trl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9864, -0.9840, -0.9868,  ..., -0.9807, -0.9875, -0.9830],\n",
       "          [-0.9866, -0.9884, -0.9857,  ..., -0.9836, -0.9876, -0.9815],\n",
       "          [-0.9836, -0.9884, -0.9817,  ..., -0.9873, -0.9839, -0.9866],\n",
       "          ...,\n",
       "          [-0.9368, -0.9381, -0.9355,  ..., -0.9192, -0.9240, -0.9374],\n",
       "          [-0.9377, -0.9315, -0.9284,  ..., -0.9344, -0.9275, -0.9324],\n",
       "          [-0.9320, -0.9199, -0.9301,  ..., -0.9380, -0.9394, -0.9433]]]),\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=512x512>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[100][0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[100][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
