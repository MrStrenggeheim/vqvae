model:
  n_channel: 1
  n_hiddens: 128
  n_residual_hiddens: 32
  n_residual_layers: 2
  n_embeddings: 1024 # was 512. but taking 32x32 = 1024
  embedding_dim: 32 # was 64. but taking 32 bc why not
  beta: 0.25

dataset:
  path: "/vol/aimspace/projects/practical_WS2425/diffusion/data/amos_robert_slices/"
  train_on_labels: True # whether to use images or labels for autoencoder
  image_size: 512
  index_range: [0, 500] # Only CT images
  slice_range: Null
  only_labeled: False # only take slices with at least one labeled pixel
  batch_size: 16 # was 64 

training:
  learning_rate: 0.0001
  max_epochs: 100
  early_stopping_patience: 10
  validation_interval: .5
  seed: 0

logging:
  experiment_name: "vqvae_labels"
  log_dir: "runs/"
  log_interval: 10
